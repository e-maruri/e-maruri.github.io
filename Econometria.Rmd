---
title: "Econometría"
output:
  html_document:
    toc: true
    toc_depth: 3
header:
  - \usepackage{amsmath}
  - \usepackage[spanish]{babel}
  - \selectlanguage{spanish}
---

```{r, echo=FALSE}
pacman::p_load(tidyverse, 
               lubridate, 
               stargazer,
               knitr,
               kableExtra,
               MASS)
```

# Econometría 
Primero creamos una muestra aleatoria i.i.d:
$$
\begin{Bmatrix} \begin{pmatrix}  Y_i \\ X_{1i}\\ X_{2i} \end{pmatrix} \end{Bmatrix}_{i=1}^{400} \sim N_3 \begin{bmatrix} \begin{pmatrix}  1 \\ 0\\ 2 \end{pmatrix}, \begin{pmatrix}  0.8 & 0.4 & -0.2 \\ 0.4 & 1.0 & -0.8 \\ -0.2 & -0.8 & 2.0 \end{pmatrix}  \end{bmatrix}.
$$ 
```{r}
#Para los parámetros
mu <- c(100,0,2)
sigma <- matrix(c(0.8, 0.4, -0.2, 0.4, 1, -0.8, -0.2, -0.8, 2), nrow = 3, byrow = T)
#Para general la muestra 
MA <- mvrnorm(400,mu,sigma) #mvrnorm de la libreria MASS
Y <- MA[,1] 
X <- cbind(rep(1,400),MA[,2:3])
```

Sabemos que el estimador de minimos cuadrados ordinarios es $\hat b_{MCO}=(X^{\prime}X)^{-1}X^{\prime}Y$, por lo que ahora podemos calcularlo fácilmente.

```{r}
(b_mco <- solve(t(X)%*%X)%*%t(X)%*%Y)
```

## Propiedades estadísticas de los estimadores

De esta forma el modelo $Y_i=0.6+0.9X_{1i}+0.4X_{2i}+U_i$ cumple con los supuestos del modelo de regresión lineal clásico, es decir: 
\begin{enumerate}
  \item linealidad en los parámetros;
  \item observaciones provenientes de una muestra aleatoria;
  \item no colinealidad perfecta entre variables explicativas; 
  \item media condicional del término no observado igual a cero; 
  \item heterocedasticidad; y
  \item normalidad del término no observado, con media cero y varianza constante. 
\end{enumerate}


### Insesgadez
Generamos una muestra i.i.d $\begin{pmatrix} X_{1i} \\ X_{2i} \end{pmatrix}_{i=1}^{400} \sim N \begin{pmatrix} \begin{pmatrix} 13 \\ 8 \end{pmatrix}, \begin{pmatrix} 4 & 0 \\ 0 & 4.5 \end{pmatrix} \end{pmatrix}$ de variables aleatorias i.i.d.
```{r}
#Para plantar una "semilla" y se generen siempre los mismos números aleatorios
set.seed(2020)
#Para los parámetros de la muestra
mu <- c(13,8)
sigma <- matrix(c(4, 0, 0, 4.5), nrow = 2, byrow = T)
MA <- mvrnorm(400,mu,sigma)
```

Ahora generamos una muestra i.i.d. $\{U_i\}_{i=1}^{400}$ de v.a.s i.i.d donde $U_i \sim N(0,1)$, $i=1,2,...,400$ tal que $U_i$ y $\mathbf{X}_i$ son independientes $\forall i \neq j$ , $i,j \in  \{1,2,...,400\}$.
```{r}
U <- rnorm(400,mean=0,sd=1)
```

Con estos datos generamos la muestra $\{Y_i\}_{i=1}^{400}$  donde $Y_i=0.6+0.9X_{1i}+0.4X_{2i}+U_i$ , $i=1,2,...,400$.
```{r}
Y <- matrix(0, ncol=1, nrow=400)
for(i in 1:400) {
  Y[i,] <- 0.6 + 0.9*MA[i,1] + 0.6*MA[i,2] + U[i]
}
```

Ahora calculamos el estimador $\hat{b}_{MCO}=\begin{pmatrix}  \hat \beta_{0} \\ \hat \beta_{1} \\ \hat \beta_{2} \end{pmatrix}$: 
```{r}
#Primero formamos una matriz con un vector de unos al inicio para la constante 
X_1 <- cbind(rep(1,400),MA[,1],MA[,2])
#Ahora sí estimamos b_mco
b_mco <- solve(t(X_1)%*%X_1)%*%t(X_1)%*%Y
#Para presentar los resultados
rownames(b_mco) <- c("$\\beta_1$","$\\beta_2$","$\\beta_3$")
kable(b_mco, caption = "Resultados", digits = 3)
```

